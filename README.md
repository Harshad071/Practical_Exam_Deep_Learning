# Project Title  
**News Headline Generator using LSTM, Attention, and Self-Attention**

---

##Objective  
The goal of this project is to automatically generate short, meaningful news headlines from longer news articles using deep learning. The project compares three different approaches based on the LSTM (Long Short-Term Memory) architecture:

- **Basic LSTM (without attention)**  
- **LSTM with Attention**  
- **LSTM with Self-Attention**

These models are evaluated using standard NLP metrics like **ROUGE** and **BLEU**.

---

##Model Architectures (Theoretical Overview)

### ðŸ”¹ 1. LSTM without Attention  
- This model uses a basic encoder-decoder structure.  
- The encoder reads the input article and compresses it into a single fixed vector.  
- The decoder tries to generate the headline from this compressed representation.  
- Since all the input is compressed into one vector, it often loses important information, especially in longer articles.  
- This model usually struggles with longer or more complex sequences.

---

### 2. LSTM with Attention  
- This version improves the basic LSTM model by allowing the decoder to "attend" to specific parts of the input while generating each word of the headline.  
- Instead of relying on a single compressed vector, the decoder dynamically focuses on the most relevant words in the input article at every step.  
- This makes the model more accurate, especially in capturing important details.  
- Headlines generated by this model are typically more contextual and informative.

---

### 3. LSTM with Self-Attention  
- Self-Attention allows the model to analyze the relationships between all words in the input, not just the immediate neighbors.  
- It helps the model determine which words in the article are most important globally.  
- This approach is inspired by the Transformer architecture and works especially well with longer texts.  
- Compared to standard attention, self-attention is more flexible and can capture complex dependencies between distant words.

---

##Evaluation Metrics (Theoretical Explanation)

To evaluate how well the models generate headlines, two widely used NLP metrics are used:

###1. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)  
- ROUGE measures how much of the original (real) headline is covered by the generated one.  
- It checks for overlaps in individual words (ROUGE-1), word pairs (ROUGE-2), and longer sequences (ROUGE-L).  
- It focuses on **recall**, meaning it measures how much of the correct content the model was able to recover.  
- Higher ROUGE scores mean that the generated headline is more similar to the real one.

---

###2. BLEU (Bilingual Evaluation Understudy)  
- BLEU focuses on how many parts of the generated headline match the actual headline.  
- It checks for matching words and phrases (like 1-word, 2-word, etc.).  
- BLEU emphasizes **precision**, which means how accurate and fluent the generated text is.  
- It also penalizes very short or repetitive headlines to ensure quality.

---
###Dataset Link

- https://www.kaggle.com/datasets/sunnysai12345/news-summary
---
